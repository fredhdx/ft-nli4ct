{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6542e2-df5a-46ec-925a-1cb065a73408",
   "metadata": {},
   "source": [
    "# Quick Startup\n",
    "\n",
    "```Bash\n",
    "# enter runtime dir\n",
    "cd code/\n",
    "\n",
    "# install deps\n",
    "pip3 install -r requirements.txt\n",
    "\n",
    "# edit parameters (see Code and Experiment Setup section)\n",
    "vim main.py\n",
    "\n",
    "# run\n",
    "python3 main.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24c602-d4eb-4870-a956-b3031e0d2923",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Movitation\n",
    "Clinical trials are essential for experimental medicine because they assess the safety and efficacy\n",
    "of novel medical treatments and ensure their compliance with regulations[1][4]. Clinical Trial\n",
    "Reports (CTR) are important tools that record trial methodologies and results, and are pivotal\n",
    "in evaluating trail effectiveness [1][2]. As of 2020, there were over 400,000 public CTRs, with 130\n",
    "CTRs published daily, making it impractical for medical practitioners to perform a comprehensive\n",
    "study before prescribing new treatments to patients [5].\n",
    "\n",
    "Natural Language Inference (NLI), which uses language models to understand documents and conduct \n",
    "subsequent tasks, is one appealing solution. It was the topic of the 2023 SEMEVAL Task 7 Challenge:\n",
    "Multi-evidence Natural Language Inference for Clinical Trial Data (NLI4CT), where different models \n",
    "were proposed to solve first textual entailment then evidence retrieval tasks for a set of breast \n",
    "cancer CTRs and expert-annotated labels[5]. The entailment task was considered more challenging because\n",
    "the models needed to perform multi-hop reasonings, numerical understanding, biomedical understanding and \n",
    "common-sense reasoning. Out of 643 submissions for the entailment task, a fine-tuned *Flan-T5* LLM \n",
    "scored the second place[5].\n",
    "\n",
    "### Task Description\n",
    "\n",
    "The entailment task targeted in the project can be described by Figure 1. Given a CTR premise and an expert-\n",
    "prepared medical statement, the model needs to predict the inference relation between the premise and the\n",
    "statement (hypothesis) as either *Contradiction* or *Entailment*. In the original task (2023), a section ID was\n",
    "also provided so only the relevant section of the CTR premise was used. In our project, there will be no\n",
    "Section ID so the entire CTR premise will be used.\n",
    "\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/task_description.png\" alt=\"Task Description\" style=\"width: 400px;\"/>\n",
    "    <div><text>Figure 1. Task Description</text></div>\n",
    "    <br/>\n",
    "</div>\n",
    "\n",
    "The reason for removing the Section IDs is that annotation Section IDs is a rather expensive \n",
    "task and not always available in real life scenarios. By removing the Section IDs, we could generalize our \n",
    "approaches to meet more realistic use cases and hopefully provide more values. \n",
    "\n",
    "By removing Section IDs, we subject our task to new challenges and will answer the following questions:\n",
    "\n",
    "1. **Is Section ID necessary to achieve good accuracy?**\n",
    "2. **Can we improve accuracy using full CTR as premise?**\n",
    "3. **Can we operate using limited input token length?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf3b17-f4bd-4b5a-8501-c91bb007f9ae",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "Our approach is divided into two major categories:\n",
    "\n",
    "1. Section ID Baselines\n",
    "2. Full CTR Text Methods\n",
    "\n",
    "The Section ID baselines follow the original task workflow described in Figure 1, where a (Section Premise, Hypothesis)\n",
    "is combined into a prompt and fed into a LLM to make an entailment prediction. This method is used to establish a Section ID\n",
    "baseline and answer question 1. It is also described in Figure 2 (*). \n",
    "\n",
    "The Full CTR Text methods contain 5 different ways to handle a full CTR text as the premise, which all tries to either compress\n",
    "or extract important information from the CTR. These methods are described in Figure 2 (A)-(E). A baseline using Full CTR text \n",
    "directly as premise is also established and described in Figure 2 (*).\n",
    "\n",
    "A detailed explanation for each method is provided in the next sub-section.\n",
    "\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/methods_description.png\" alt=\"Method Description\" style=\"width: 800px;\"/>\n",
    "    <div><text>Figure 2. Methods Overview</text></div>\n",
    "    <br/>\n",
    "</div>\n",
    "\n",
    "## Methods Explanation\n",
    "\n",
    "**(*) Baselines:** A premise (either Section Extraction or Full Text) is combined with the hypothesis and fed into the \n",
    "LLM using a prompt (shown next sub-section). The LLM then makes a zero-shot inference, predicting the inference relation\n",
    "between the premise and the hypothesis as either *Contradiction* or *Entailment*.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "Prompt &= Prompt\\_Template(premise, hypothesis)\\\\\n",
    "Prediction &= LLM(Prompt)\n",
    "\\end{aligned}$$\n",
    "\n",
    "**(A) Top K Sentence Retrieval:** Sentence embeddings for all CTRs in dataset are pre-calculated using *all-MiniLM-L6-v2* \n",
    "model into a vector databse. When a (CTR, hypothesis) is provided, the hypothesis is first converted into a sentence embedding\n",
    "using the same embedding model, which is used to search within the CTR vector database to find K most relevant CTR sentences\n",
    "using cosine similarity. The top K sentences were then retrieved, concatenated together to form a *new premise*. The new premise\n",
    "and the hypothesis is then combined into a prompt and fed into the LLM for entailment.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\vec{h} &= Embed(Hypothesis)\\\\\n",
    "Similarity\\_matrix &= cosine\\_similarity(h\\_vector, all\\_CTR\\_sentence\\_vectors)\\\\\n",
    "Top\\_K\\_sentences &= Find\\_max(Similarity\\_matrix, K, all\\_CTR\\_sentences)\\\\\n",
    "premise &= concat(Top\\_K\\_sentences)\\\\\n",
    "Prediction &= LLM(Prompt\\_Template(premise, hypothesis))\n",
    "\\end{aligned}$$\n",
    "\n",
    "**(B) Sliding Window:** Choosing a *MODEL_TOKEN_LIMIT as window_size* and a *STRIDE as stride_size*, the full CTR text is splitted into chunks by sliding a fixed-size window along itself. Each chunk is treated as a *chunk premise* and combined with the hypothesis to make a *chunk prediction* using the prompt and LLM. After obtaining all chunk predictions, we count the number of each classes (*Contradiction*, *Entailment*) and use the maximum count class as the final entailment label.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&chunks = Split(fullCTRPremise, window\\_size, stride\\_size)\\\\\n",
    "&chunk\\_predictions = [\\quad]\\\\\n",
    "&for\\quad chunk\\_premise\\quad in\\quad chunks:\\\\\n",
    "&\\quad chunk\\_predictions.add(LLM(Prompt\\_Template(chunk_premise, hypothesis)))\\\\\n",
    "&Prediction =  argmax(chunk\\_predictions)\n",
    "\\end{aligned}$$\n",
    "\n",
    "**(C) LLM Summarization:** Choosing a *SUMMARY_GEN_MAX_TOKEN* and a *MODEL_TOKEN_LIMIT*, we use the same LLM to generate a summary of \n",
    "maximum length *SUMMARY_GEN_MAX_TOKEN* for each *SECTION* of the full CTR premise. Each section summary is then concatenated together\n",
    "to form a final premise, and used with the hypothesis to generate entailment prediction. \n",
    "\n",
    "When generating section summary, section text is split into chunks IF the size is over *MODEL_TOKEN_LIMIT*. Each chunk is used to generate\n",
    "a chunk summary and combined together either by *concatenatin* or *a secondary summary using LLM performed on concatenated chunk summaries* to generate the final *section summary*. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "&section\\_summaries = [\\quad]\\\\\n",
    "&for\\quad section\\_id\\quad in\\quad fullCTRPremise\\\\\n",
    "&\\quad section\\_text = extract(fullCTRPremise, section\\_id)\\\\\n",
    "&\\quad if\\quad len(section\\_text) > MAX\\_INPUT\\_TOKEN:\\\\\n",
    "&\\quad\\quad chunks = Split(section\\_text, MAX\\_INPUT\\_TOKEN)\\\\\n",
    "&\\quad\\quad\\quad chunk\\_section\\_summaries = LLM(Summary\\_Prompt\\_Template(chunk\\_text))\\\\\n",
    "&\\quad\\quad\\quad section\\_summary = concat(chunk\\_section\\_summaries)\\\\\n",
    "&\\quad\\quad\\quad\\quad\\quad\\quad OR\\\\\n",
    "&\\quad\\quad\\quad section\\_summary = LLM(Summary\\_Prompt\\_Template(concat(chunk\\_section\\_summaries)))\\\\\n",
    "&\\quad else:\\\\\n",
    "&\\quad\\quad section\\_summary = LLM(Summary\\_Prompt\\_Template(section\\_text))\\\\\n",
    "&\\quad section\\_summaries.add(section\\_summary)\\\\\n",
    "&CTR\\_Summary = concat(section\\_summaries)\\\\\n",
    "&Prediction = LLM(Prompt\\_Template(CTR\\_Summary, hypothesis))\n",
    "\\end{aligned}$$\n",
    "\n",
    "**(D) Premise Truncation:** Choosing a *MODEL_TOKEN_LIMIT*, we truncate the full CTR premise and use the truncated premise and hypothesis to predict entailment label.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "Truncated\\_Premise &= Truncate(fullCTRPremise, MAX\\_INPUT\\_TOKEN)\\\\\n",
    "Prediction &= LLM(Prompt\\_Template(Truncated\\_Premise, hypothesis))\n",
    "\\end{aligned}$$\n",
    "\n",
    "**(E) Section ID Retrieval:** Similar to original Section ID workflow, we use the same LLM to first predict relevant Section ID given the full CTR premise and the hypothesis. We then retreive section premise using the predicted Section ID, and use it together with the hypothesis to predict entailment label.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "Section\\_ID &= LLM(Predict\\_SECTION\\_Prompt\\_Template(fullCTRPremise, hypothesis))\\\\\n",
    "Premise &= Extract(fullCTRPremise, Section\\_ID)\\\\\n",
    "Prediction &= LLM(Prompt\\_Template(Premise, hypothesis))\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Some important conditions:**\n",
    "\n",
    "1. Without otherwise mentioned, the full CTR premise is created by concatenating sentences from each section together, along with the Section ID as an added title in between. The concatenation order is always \"intervention\", \"eligibility\", \"adverse events\", and \"results\", the same order presented in original CTR file. An glimpse of full CTR premise is shown below:\n",
    "```Python\n",
    "\"\"\"\n",
    "  intervention\\nIntervention Sentence 1\\nIntervention Sentence 2\\nIntervention Sentence 3\\nIntervention Sentence 1...\\n\n",
    "  eligibility\\nEligibility Sentence 1\\nnEligibility Sentence 1\\nEligibility Sentence 2\\nEligibility Sentence 3\\n\n",
    "  adverse events\\nAdverse Events Sentence 1\\nAdverse Events Sentence 2\\nAdverse Events Sentence 3\\n\n",
    "  results\\nResults Sentence 1\\nResults Sentence 2\\nResults Sentence\\n\n",
    "\"\"\"\n",
    "```\n",
    "2. In *Top K Sentence Retrieval*, the sentence embeddings are created in the same order they appear in the full CTR premise. However, in Top K search, all sentences from all sections are compared with hypothesis and any sentence could be retrieved.\n",
    "\n",
    "3. In *Top K Sentence Retrieval*, the pre-calculated vector database (pickle files) should be provided and loaded into the program using parameter `TOPK_DB` in `main.py, line 255`. This should be the relative path pointed to the location where two files exist: `raw_text_db.pickle` and `annotations_db_val.pickle`. They both must exist with exact same file names. Please refer to Setup Section for more information.\n",
    "\n",
    "4. While we use the same LLM model for entailment, summarization, and section prediction, you could replace them by changing the code.\n",
    "\n",
    "## Model Choice\n",
    "**Flan-T5** is chosen as our base infernce model. It is chosen for two reasons: 1. It was proven to work well on \n",
    "the NLI4CT entailment task compared to other LLMs[6]. 2. It is relatively small (780M parameters for large and 3B \n",
    "for XL vs 7 and 11 B for LLaMA). 3. It has been instruction fine-tune to work well with zero-shot inference[3].\n",
    "\n",
    "**all-MiniLM-L6-v2** is chosen for sentence embedding for one of our methods (Top K). It is a well-known sentence transformer\n",
    "pretrained model with fast inference speed and good quality.\n",
    "\n",
    "## Prompt Template\n",
    "\n",
    "Three prompt templates will be used throughout our experiments. \n",
    "\n",
    "### Entailment Prompt\n",
    "\n",
    "This prompt is taken out from the top LLM paper in 2023 NLI4CT challenge[6]. The author experimented with multiple prompt templates for Flan-T5 and published the best working one[6]. The prompt takes in two parameters: a **CTR Premise** which is either the Section premise, full CTR premise, or augmented premise by one of the five methods we experiment with, and a **statement**, which is taken directly out of each sample/instance. The *Options* line is added to guide the instruction-tuned model to produce only expected output.\n",
    "\n",
    "```Python\n",
    "\"\"\"\n",
    "{CTR Premise}\n",
    "Question: Does this imply { statement }?\n",
    "Options: entailment , contradiction\"\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### Summarization Prompt\n",
    "\n",
    "We crafted the following prompt for Flan-T5 to generate a text summary given a premise (long text). We ask the LLM to include details and control the length of maximum generation using a parameter.\n",
    "\n",
    "```Python\n",
    "\"\"\"\n",
    "Please summarize the following and include important details as much as possible: {}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### Section ID Prediction Prompt\n",
    "\n",
    "We crafted a prompt for Flan-T5 to predict the section ID given full CTR text and the pairing hypothesis. We instructed the LLM to pay attention to four sections and provides a template to generate a relevant response.\n",
    "\n",
    "```Python\n",
    "\"\"\"\n",
    "A premise contains four sections: {}\\nA hypothesis describes one of the sections: {}\\nDetermine the most relevant section from the four options: intervention, results, eligibility, adverse_events\".\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db83280e-aaab-4610-86cf-fdab8e8b4ecb",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "*Details on how to load and use the dataset is provided in Experimental Setup section.*\n",
    "\n",
    "## Source\n",
    "In our project, we use the data from [NLI4CT 2024](https://sites.google.com/view/nli4ct/semeval-2024). The official dataset could be found at this [Github Link](https://github.com/ai-systems/Task-2-SemEval-2024/tree/main). A community-built Transformer Dataset is also available at [Huggingface Link](https://huggingface.co/datasets/bigbio/sem_eval_2024_task_2/tree/main). We used the latter for convenience.\n",
    "\n",
    "In case the community dataset is no longer available, please go to the official link and prepare your data into (annotations, id_to_clinical_trial_record) tuple. For more information, read `code/main.py/load_data()` function.\n",
    "\n",
    "In case you want to try the dataset yourself:\n",
    "```Python\n",
    "import datasets\n",
    "annotations = datasets.load_dataset(\"bigbio/sem_eval_2024_task_2\", name=\"sem_eval_2024_task_2_source\")\n",
    "raw_texts = datasets.load_dataset(\"bigbio/sem_eval_2024_task_2\", name=\"sem_eval_2024_task_2_ct\")[\"train\"]\n",
    "```\n",
    "\n",
    "## Dataset Description\n",
    "The NLI4CT dataset is based on a collection of 1000 publicly available breast cancer **CTRs** on [ClinicalTrials.gov](https://clinicaltrials.gov/ct2/home), expert annotated **statements**, and **labels**[5]. \n",
    "\n",
    "Each CTR (premise) is separated into four sections: intervention, eligibility, results and adverse events. Each section consists of a list of medical sentences. Each statement (hypothesis) is a single sentence making a claim about one or two CTRs, depending on the annotation type being either *Single* or *Comparison*. Each label states the inference relation between the statement and the CTRs. The entire dataset is split into train, dev, test sets by 1700, 200, 500 respectively[5]. \n",
    "\n",
    "**In our project, we use only the `dev` set because `test` set is not public and `train` set is too large for our limited resources.**\n",
    "\n",
    "When working with the dataset, you will have two files: the CTR file and the annotation file. We provide examples for each of them below.\n",
    "\n",
    "### CTR Sample\n",
    "```json\n",
    "{\n",
    "    \"Clinical Trial ID\": \"NCT03374995\",\n",
    "    \"Intervention\": [\n",
    "        \"INTERVENTION 1: \",\n",
    "        \"  Group I (Topical Keratin)\",\n",
    "        \"  Patients receive topical keratin topically at least BID until the end of radiation therapy (approximately 3-6 weeks).\"\n",
    "    ],\n",
    "    \"Eligibility\": [\n",
    "        \"Inclusion Criteria:\",\n",
    "        \"  Area to be irradiated representing 1-10% of total body surface area (TBSA)\",\n",
    "        \"Exclusion Criteria:\"\n",
    "    ],\n",
    "    \"Results\": [\n",
    "        \"Outcome Measurement: \",\n",
    "        \"  Incidence of Early Adverse Skin Reactions (EASRs)\",\n",
    "        \"Results 1: \",\n",
    "        \"  Arm/Group Title: Group I (Topical Keratin)\"\n",
    "    ],\n",
    "    \"Adverse Events\": [\n",
    "        \"Adverse Events 1:\",\n",
    "        \"  Total: 0/13 (0.00%)\",\n",
    "        \"Adverse Events 2:\",\n",
    "        \"  Total: 0/11 (0.00%)\"\n",
    "    ]\n",
    "```\n",
    "\n",
    "Here, \"Clinical Trial ID\" refers to the unique ID of this CTR file. Each section is represented as a list of sentences. The number of sentences range from very few (2 or 3) to large (over 30). Each section contains on average 5-500 tokens[5].\n",
    "\n",
    "### Annotation Sample\n",
    "```json\n",
    "{\n",
    "    \"1adc970c-d433-44d0-aa09-d3834986f7a2\": {\n",
    "        \"Type\": \"Single\",\n",
    "        \"Section_id\": \"Results\",\n",
    "        \"Primary_id\": \"NCT00066573\",\n",
    "        \"Statement\": \"there is a 13.2% difference between the results from the two the primary trial cohorts\",\n",
    "        \"Label\": \"Contradiction\",\n",
    "        \"Primary_evidence_index\": [0, 1, 23, 4, 23]\n",
    "    }\n",
    "```\n",
    "\n",
    "Each annotation represents a data sample for the entailment task. It contains expert-annotated class label, the section ID and the statement (hypothesis). Here, \"Type\" refers to whether the annotation looks at a single CTR or a pair of CTR. When looking at a pair, the statement makes a claim about both of the CTRs (usually as a comparison). The \"Primary_id\" and \"Secondary_id\", if applicable, represent the \"Clinical Trial ID\" of relevant CTRs. The \"Primary_evidence_index\" and \"Secondary_evidence_index\", if applicable, are correct retrieval sentence indices from relevant section for evidence retrieval task, which is not applicable in our project. \"Section_id\", \"Label\" and \"Statement\" are self-explanatory. \n",
    "\n",
    "### Data Balance\n",
    "\n",
    "The dataset (dev) contains 200 samples and is overall balanced. \n",
    "\n",
    "+ There are exactly 100 Contradication samples and 100 Entailment samples.\n",
    "+ There are similar number of samples for each annotated section.\n",
    "+ There are more single samples than comparison samples.\n",
    "\n",
    "| Type |  | Section |  |  |  | Label |  |\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|  | Count | Intervention | Eligibility | Adverse Events | Results | Contradiction | Entailment |\n",
    "| Single | 140 | 26 | 44 | 32 | 38 | 70 | 70 |\n",
    "| Comparison | 60 | 10 | 12 | 20 | 18 | 30 | 30 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665aacf-b220-4f29-95f4-47651fe5207b",
   "metadata": {},
   "source": [
    "# Code \n",
    "\n",
    "**We programed our own inference / evaluation code from scratch** using PyTorch and HuggingFace Transformer packages. Our code supports **commandline executation** with parameter setting within the executable script. The code directory structure is shown below:\n",
    "\n",
    "```\n",
    "- Root Directory/\n",
    "  - code/\n",
    "    - main.py               -- IMPORTANT: executable eval script\n",
    "    - functions.py          -- IMPORTANT: primary model class and help functions\n",
    "    - topksearcher.py       -- IMPORTANT: top k search class\n",
    "    - vectordb/              -- IMPORTANT: location of pre-calculated embedding binaries\n",
    "      - allMiniLML6V2\n",
    "        - annotations_db_val.pickle  -- IMPORTANT: embedding binary for all annotation statements\n",
    "        - raw_text_db.pickle          -- IMPORTANT: embedding binary for all CTR files\n",
    "    - prepare-topk-embeddings.ipynb  -- IMPORTANT: use this to generate embedding binaries\n",
    "    - f1-calc.ipynb                  -- Script to calculate precision, recall, F1\n",
    "    - prepare-section-id.ipynb       -- Script to test section id prediction\n",
    "    - requirements.txt\n",
    "    - Final_Project.ipynb            -- Playground notebook for code development.\n",
    "\n",
    "  - large_results/         -- experiment result logs for large model\n",
    "  - xl_results/            -- experiment result logs for xl model\n",
    "  - f1_calculation/        -- sanitized logs prepared for precision-recall calculation for project submission\n",
    "```\n",
    "\n",
    "## Environment Setup\n",
    "```bash\n",
    "cd code\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "We recommend using `conda` to create a fresh `python3.11` environement first.\n",
    "\n",
    "You will need enough disk space to download datasets and models as well.\n",
    "\n",
    "## Run Inference\n",
    "\n",
    "To run the experiment, all you need to do is to set up parameters in `main.py`. If you want to use `topk` method, make sure you have correctly prepared embedding database (see Biuld Embedding Binaries section).\n",
    "\n",
    "```bash\n",
    "python3 main.py\n",
    "```\n",
    "\n",
    "### Experiment Parameters\n",
    "Our `main.py` script supports all methods listed in our Approach Section and their associated parameters tuning directly in file. The setup section starts at `LINE 274`. To understand these parameteres, please refer to the comments as well as the docstring at `LINE 70`. \n",
    "\n",
    "On a high level, you can choose between using section ID or not (full CTR) for building the premise, and 6 different ways for augmenting the premise. A compatability matrix is provided below:\n",
    "\n",
    "|  | base | truncate | sliding_window | summarize-concat | summarize | topk | autosection |\n",
    "|---|---|---|---|---|---|---|---|\n",
    "| Section Mode | Yes | Yes | Yes | No | No | No | Yes |\n",
    "| Full CTR Mode | Yes | Yes | Yes | Yes | Yes | Yes | Yes |\n",
    "\n",
    "*autsection is the code name for section retrieval method.*\n",
    "\n",
    "*summarize-concat and summarize are both summarizaiton method. The difference is how to combine chunks when building section summary.*\n",
    "\n",
    "*topk requires existence of embedding binaries.*\n",
    "\n",
    "### Logging\n",
    "\n",
    "**Logging is an important part of the program and required for successful recording of experiment results.**\n",
    "\n",
    "Logs are automatically created when running the script and two logs files are created. `{expname}_result.log` contains prediction results. `{expname}.log` contains runtime logs. The reusult log is a dictionary file:\n",
    "\n",
    "```python\n",
    "{'acc': pred_accuracy, 'result': [[pred_label, true_label]], 'logs': \"additional logs for some methods\"}\n",
    "```\n",
    "\n",
    "You need to set up the `expname` in `main.py, LINE 298`, `logname = f\"{expname}\"`. If you use the same name from previous experiments, logs will be overwritten. By default, the name is using relative path to `main.py`.\n",
    "\n",
    "Cmd-line output is the same as from `{expname}.log`.\n",
    "\n",
    "### Parameter Examples For Different Experiments\n",
    "\n",
    "**Shared Parameters**\n",
    "\n",
    "These parameters are used with all methods.\n",
    "\n",
    "```python\n",
    "MODEL_NAME = \"xl\" # Flan-T5 model size. Check this page for available models: https://huggingface.co/docs/transformers/model_doc/flan-t5\n",
    "INCLUDE_ID = True # Whether to include section id names when building full CTR premise. Refer to Model Explanation section. We use True for all our experiments.\n",
    "RANDOM_SECTION = False # (Experimental) Whether to choose a random section (Section mode) or a random section orders (Full CTR mode) when building full CTR premise.\n",
    "USE_SECTION = True # whether to use section mode or full CTR mode. Each is compatible with a selected methods\n",
    "```\n",
    "\n",
    "**Section ID Baseline**\n",
    "```python\n",
    "USE_SECTION = True\n",
    "PREMISE_METHOD = \"base\" # Chooses what type of method to handle premise. For section mode, only supports \"base\", \"sliding_window\", \"truncate\", \"autosection\". \"autosection\" is the name for section retrieval.\n",
    "```\n",
    "\n",
    "**Full CTR Baseline**\n",
    "```python\n",
    "USE_SECTION = False\n",
    "PREMISE_METHOD = \"base\"\n",
    "```\n",
    "\n",
    "**Full CTR Truncation**\n",
    "```python\n",
    "USE_SECTION = False\n",
    "PREMISE_METHOD = \"truncate\"\n",
    "MODEL_TOKEN_LIMIT = 512 # unit: token\n",
    "```\n",
    "\n",
    "**Full CTR Sliding Window**\n",
    "```python\n",
    "USE_SECTION = False\n",
    "PREMISE_METHOD = \"sliding_window\"\n",
    "MODEL_TOKEN_LIMIT = 512 # unit: token\n",
    "STRIDE = 256 # unit: token\n",
    "```\n",
    "\n",
    "**Full CTR Summarization with Concat**\n",
    "```python\n",
    "USE_SECTION = False\n",
    "PREMISE_METHOD = \"summarize-concat\"\n",
    "MODEL_TOKEN_LIMIT = 512 # unit: token\n",
    "SUMMARY_GEN_MAX_TOKEN = 380 # unit: token\n",
    "```\n",
    "\n",
    "**Full CTR Hierarchical Summarization**\n",
    "```python\n",
    "USE_SECTION = False\n",
    "PREMISE_METHOD = \"summarize\"\n",
    "MODEL_TOKEN_LIMIT = 512 # unit: token\n",
    "SUMMARY_GEN_MAX_TOKEN = 380 # unit: token\n",
    "```\n",
    "\n",
    "**Full CTR Top K**\n",
    "```python\n",
    "USE_SECTION = False\n",
    "PREMISE_METHOD = \"topk\"\n",
    "TOPK = 50\n",
    "TOPK_DB = \"./vectordb/allMiniLML6V2/\" # important, must exist with two pickle files: raw_text_db.pickle, annotations_db_val.pickle\n",
    "```\n",
    "\n",
    "**Full CTR Section Retrieval**\n",
    "```python\n",
    "USE_SECTION = False\n",
    "PREMISE_METHOD = \"autosection\"\n",
    "```\n",
    "\n",
    "### (IMPORTANT) Build Embedding Binaries\n",
    "To build embedding binaries, run `prepare-section-id.ipynb`.\n",
    "\n",
    "By default, it will use `sentence-transformers/all-MiniLM-L6-v2` model from HuggingFace. \n",
    "\n",
    "Run the section `embed raw_text and annotation sentences`.\n",
    "\n",
    "At the end of the section, you will save to pickle files to the current dir, `raw_text_db.pickle, annotations_db_val.pickle`. DO NOT change their names but do move them to a place where you want to store them. By default, we put it under `code/vectordb/allMiniLML6V2`. If you decide to change the location, make sure you update your `TOPK_DB` parameter in `main.py`.\n",
    "\n",
    "### Calcualte Precision-Recall\n",
    "We also provided a notebook `f1-calc.ipynb` to help you process experiment resulting log files and calculate f1 scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6aa8ef-3b85-4a92-b1cc-d615ce4c9248",
   "metadata": {},
   "source": [
    "# Experimental Setup\n",
    "\n",
    "We ran the following sets of experiments to compare 1) Section Baseline and Full CTR Text Baseline 2) Five different full text handling methods against Full CTR Text Baseline 3) Methods among each other. Our primary metric is accuracy while we also calculated precision, recall for some experiments. Below is a list of experiments we have run:\n",
    "\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/list_of_experiments.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "    <br/>\n",
    "</div>\n",
    "\n",
    "## Evaluation Metrics\n",
    "Accuracy is used as the primary metric. We calculate precision, recall and macro-F1 scores for some cases as well.\n",
    "\n",
    "$Accuracy = {True\\ Positive\\ +\\ True\\ Negative \\over (True\\ Positive\\ +\\ True\\ Negative\\ +\\ False\\ Positive\\ +\\ False\\ Negative)}$\n",
    "\n",
    "$Precision = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}$\n",
    "\n",
    "$Recall = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$\n",
    "\n",
    "$F1\\ Score = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac0ae8e-af7a-436e-a4e2-2bf55a77652c",
   "metadata": {},
   "source": [
    "# Results\n",
    "## 1. Baseline Comparison\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/result_sectionID_effectiveness.png\" alt=\"Effectiveness of Section ID\" style=\"width: 400px;\"/>\n",
    "    <div><text>Figure 3: Zero-shot performance of FlanT5-large and XL on premise with a) only true\n",
    "section, b) full CTR, and c) a random section, compared against previous work with true\n",
    "section on FlanT5-XXL</text></div>\n",
    "    <br/>\n",
    "</div>\n",
    "\n",
    "## 2. Optimizing Full Text Prediction \n",
    "We tried 5 methods to handle the lengthy text by reducing the premise, their performance compared with the baseline are shown below:\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/results_performance.png\" alt=\"Performance of Full Text Handling Methods\" style=\"width: 400px;\"/>\n",
    "    <div><text>With Section Retrieval Success Rate: 0.88</text></div>\n",
    "    <div><text>Figure 4: Performance of Full Text Handling Methods</text></div>\n",
    "    <br/>\n",
    "</div>\n",
    "\n",
    "Their precision and recall for both tasks and sizes of models are compared:\n",
    "\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/results_precision_recall.png\" alt=\"Performance of Full Text Handling Methods\" style=\"width: 400px;\"/>\n",
    "    <div><text>Figure 5: Precision and Recall of Full Text Handling Methods</text></div>\n",
    "    <br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c52e2f-0297-4abb-a88c-2241504f47db",
   "metadata": {},
   "source": [
    "\n",
    "# Analysis of the Results\n",
    "\n",
    "\n",
    "## Effectiveness of Section ID\n",
    "In Figure 3, to evaluate the effectiveness of section ID,  we evaluated the baseline performance of zero-shot flanT5 on the task when the premise is *a) the correct section b) full text, and c) a random section*. \n",
    "* We can see that the baseline model **with section ID** performs the best overall for both large and xl, at **58.5%** and **65.5%**, respectively.\n",
    "* The performance of using full-text and random section as premise is **lower, especially the random section**, which reduced the accuracy from **58.5%** to **54.3%** for large model and **65.5%** to **56%**.\n",
    "* This demonstrates that section ID has a positive impact on the model performance. \n",
    "\n",
    "*Note in pervious work, an accuracy of 74.3% was claimed when using the XXL model on premise with correct section ID, which we did not reproduce due to the limitation of model size*\n",
    "\n",
    "\n",
    "## Performance of Full-text Handling Methods\n",
    "Of the 5 methods to handle lengthy text as premise, **Truncation and Section-retrieval** relatively out performed other methods and improved the accuracy from the full-text baseline model.\n",
    "\n",
    "In Figure 4:\n",
    "* For the large model, truncation significantly improve results **from 58.5 to 61.5** .\n",
    "* For XL model, section retrieval improved the performance by 3%, **from 62% to 65%.** This is almost on par with our baseline performance when section is given (65.5%).\n",
    "* Note the success rate of section retrieval is 88%, meaning 88% of the predicted Section ID were correct, and the performance was based on this success rate.\n",
    "\n",
    "In Figure 5, precision and recall are compared for all methods for each task (Contradiction and Entailment):\n",
    "* XL model out-performs the Large model in precision and recall, which is consistent with their accuracy performance.\n",
    "* The saparation between Contradiction and Entailment task may give insight into FlanT5's behaviour, as in, it tends to cautiously predict on contradiction statements, and greedy predicting on entailment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95ff3a-d3da-4f92-8f80-e9eb22197008",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "To answer the 3 questions we raised in Motivation:\n",
    "1. **Is Section ID necessary to achieve good accuracy?**\n",
    "\n",
    "Section ID positively impact model performance, but alternative methods can compensate for the lack of section IDs.\n",
    "\n",
    "2. **Can we improve accuracy using full CTR as premise?**\n",
    "\n",
    "Yes, some of the methods, especially section retrieval and truncation, visibly improved the performance of the baseline model\n",
    "\n",
    "3. **Can we operate using limited input token length?**\n",
    "\n",
    "Yes,  the methods also allows us to operate using limited input token length, if that is a constraint.\n",
    "\n",
    "### Why we care about limit input token size?\n",
    "1. Most large language models have a fixed maximum number of tokens they can consider at a time, commonly referred to as the model’s “context window.”\n",
    "2.  if a conversation exceeds the model’s token limit, it may forget critical information discussed earlier, resulting in out-of-context or irrelevant responses. The same applies to long documents, where the model might lose the thread of the narrative or forget key details from earlier sections. For seq2seq models, the model might foret critical information in the middle.\n",
    "3.  As the length of the sequence increases, so does the computational burden. The FlanT5 has quardratic increase of memory use as the input length.\n",
    "\n",
    "### Future Work\n",
    "\n",
    "#### Continuous Improvement of Accuracy Through Hyperparameter Tuning\n",
    "\n",
    "One potential avenue for future research could center on enhancing prediction accuracy. In our ongoing investigation, we've delved into certain parameter ranges and achieved favorable outcomes through manual tuning. We posit that employing more systematic tools, such as Weights and Biases, could enable a more exhaustive exploration of parameters, leading to further enhancements in model performance.\n",
    "\n",
    "#### Leveraging Entailment Pipeline to Perform Evidence Retrieval\n",
    "\n",
    "A natural follow-up study after our research would be using the entailment relations to extract relevant sentences from chosen CTR and support downstream tasks. \n",
    "\n",
    "For instance, we might modify the prompt to instruct the Language Model (LLM) to extract sentences that inherently support the hypothesis. Alternatively, employing a two-step approach, we could instruct the LLM to first extract relevant sections and then carry out sentence retrieval.\n",
    "\n",
    "#### Model Explainability\n",
    "\n",
    "Based on our current findings, we have made several noteworthy observations regarding model explainability that warrant further investigation. These observations include:\n",
    "\n",
    "1. Truncation Method Insights:\n",
    "\n",
    "When employing the Truncation Method, truncation occurs at the start of the full CTR text. Since our complete CTR text is a concatenation of report sections following a fixed order (\"intervention,\" \"eligibility,\" \"adverse events,\" \"results\"), achieving high accuracy implies effective comprehension by the model, even when focusing solely on the initial sections (\"intervention\" and \"eligibility\"). This insight suggests two possibilities: Firstly, the \"intervention\" segment may contain crucial information regardless of the annotated statement's source section. Secondly, there may be inherent biases among human annotators when formulating statements.\n",
    "\n",
    "2. Sliding Window Method Optimization:\n",
    "\n",
    "Notably, one of optimal performances with the Sliding Window Method was observed when utilizing a larger stride size and a smaller window size. This observation indicates that improved predictions could result from strategically skipping through certain information. We are intrigued by the prospect of distilling this skipping approach into a separate method to systematically improve accuracy.\n",
    "\n",
    "3. Precision and Recall Disparities across Label Classes:\n",
    "\n",
    "A comprehensive comparison of precision and recall for different label classes (contradiction and entailment) across all methods revealed a consistent pattern. On average, the entailment class exhibited higher recall than precision, suggesting a cautious approach by the model when predicting entailment. In contrast, the contradiction class displayed higher precision than recall, indicating a more assertive prediction tendency. We are keen to determine whether these effects are intrinsic to the dataset or specific to the FlanT5 model.\n",
    "\n",
    "These insights shed light on the inner workings of the underlying model within the specific dataset, presenting compelling avenues for further exploration in explainability studies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640d6ecd-0f4a-435c-bd8c-d1697dc75fd1",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "[1] Nancy E. Avis, Kevin W. Smith, Carol L. Link, Gabriel N. Hortobagyi, and Edgardo Rivera.\n",
    "Factors associated with participation in breast cancer treatment clinical trials. Journal of\n",
    "Clinical Oncology, 24(12):1860–1867, apr 2006.\n",
    "\n",
    "\n",
    "[2] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large\n",
    "annotated corpus for learning natural language inference. InProceedings of the 2015 Confer-\n",
    "ence on Empirical Methods in Natural Language Processing. Association for Computational\n",
    "Linguistics, 2015.\n",
    "\n",
    "[3] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\n",
    "Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\n",
    "Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie\n",
    "Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent\n",
    "Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob\n",
    "Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned\n",
    "language models, 2022.\n",
    "\n",
    "[4] Jay DeYoung, Eric Lehman, Benjamin Nye, Iain Marshall, and Byron C. Wallace. Evidence\n",
    "inference 2.0: More data, better models. InProceedings of the 19th SIGBioMed Workshop on\n",
    "Biomedical Language Processing. Association for Computational Linguistics, 2020.\n",
    "\n",
    "[5] Maël Jullien, Marco Valentino, Hannah Frost, Paul O’regan, Donal Landers, and André Freitas.\n",
    "SemEval-2023 task 7: Multi-evidence natural language inference for clinical trial data. In\n",
    "Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023).\n",
    "Association for Computational Linguistics, 2023.\n",
    "\n",
    "[6] Kamal Raj Kanakarajan and Malaikannan Sankarasubbu. Saama AI research at SemEval-\n",
    "2023 task 7: Exploring the capabilities of flan-t5 for multi-evidence natural language inference\n",
    "in clinical trial data. InProceedings of the The 17th International Workshop on Semantic\n",
    "Evaluation (SemEval-2023). Association for Computational Linguistics, 2023.\n",
    "\n",
    "[7] Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng Dou, and Ji-Rong Wen. Reta-\n",
    "llm: A retrieval-augmented large language model toolkit, 2023.\n",
    "\n",
    "[8] Keivalya Pandya and Mehfuza Holia. Automating customer service using langchain: Building\n",
    "custom open-source gpt chatbot for organizations, 2023.\n",
    "\n",
    "[9] Yuxuan Zhou, Ziyu Jin, Meiwei Li, Miao Li, Xien Liu, Xinxin You, and Ji Wu. THiFLY\n",
    "research at SemEval-2023 task 7: A multi-granularity system for CTR-based textual entailment\n",
    "and evidence retrieval. InProceedings of the The 17th International Workshop on Semantic\n",
    "Evaluation (SemEval-2023). Association for Computational Linguistics, 2023.\n",
    "\n",
    "[10] Yun Luo and Zhen Yang and Fandong Meng and Yafu Li and Jie Zhou and Yue Zhang. An Empirical Study of \n",
    "Catastrophic Forgetting in Large Language Models During Continual Fine-tuning. arXiv preprint arXiv:2308.08747, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1487b1-afe5-4ef4-9167-a7ec0946f783",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Additional Results\n",
    "\n",
    "#### Inference Speed Comparison@Large Model\n",
    "\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/results_speed_large.png\" alt=\"Performance of Full Text Handling Methods\" style=\"width: 400px;\"/>\n",
    "    <div><text>Figure 6: Inference Speed@Large</text></div>\n",
    "    <br/>\n",
    "</div>\n",
    "\n",
    "#### Increased Accuracy When Sliding Over Some Gap@XL Model?\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/results_sliding_XL.png\" alt=\"Performance of Full Text Handling Methods\" style=\"width: 400px;\"/>\n",
    "    <div><text>Figure 7: Sliding Window Performance@XL</text></div>\n",
    "    <br/>\n",
    "</div>\n",
    "\n",
    "Comment: It's interesting that when the window size is smaller than the stride size, meaning the window will skip some information, the accuracy could improve. This effect might indicate that either 1) some information in original CTR text actually confuses the model in terms\n",
    "of entailment prediction OR 2) the model happens to \"focus\" on some important information.\n",
    "\n",
    "#### Accuracy Fluctuation When Truncating at Different Size@XL Model\n",
    "<div style=\"display:inline-block;\">\n",
    "    <img src=\"imgs/results_truncation_large.png\" alt=\"Performance of Full Text Handling Methods\" style=\"width: 400px;\"/>\n",
    "    <div><text>Figure 8: Truncation Performance@XL</text></div>\n",
    "    <br/>\n",
    "</div>\n",
    "\n",
    "Comment: Accuracy is higher at low truncation size (768) and high truncation size (2000@XL, 4000@large). It's interesting because the relationship is not linear. Also, when truncation > 4000, effectively same as base case where no truncation is performed, the accuracy is \n",
    "higher than base (@XL). By further investigating the output from tokenizer, it appears that even trunction size > text size, the encoded-decoded sentence is still truncated a little (smaller length than original length). This might contribute to the performance difference and has something to do with how the Huggingface transformer encoder/decoder work internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d3b5a-5443-4cd7-8323-71e8770c22d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
