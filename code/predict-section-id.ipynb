{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import gc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from main import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'large'\n",
    "USE_SECTION = False\n",
    "INCLUDE_ID = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert CRT to premise, hypothesis and label sets\n",
    "# use_section: controls whether to extract a specific section or full text\n",
    "# include_id: controls whether to include \"section_title\" in text when extracting full text\n",
    "\n",
    "def ctr_to_full_text(ctr, include_id=False):\n",
    "  \"\"\"extract full text from ctr\n",
    "     include_id: whether to include seciton id for each section sentence list\n",
    "  \"\"\"\n",
    "  if include_id:\n",
    "    intervention = [\"intervention:\"] + ctr.get('intervention', [])\n",
    "    eligibility = [\"eligibility:\"] + ctr.get('eligibility', [])\n",
    "    adverse_events = [\"adverse_events:\"] + ctr.get('adverse_events', [])\n",
    "    results = [\"results:\"] + ctr.get('results', [])\n",
    "  else:\n",
    "    intervention = ctr.get('intervention', [])\n",
    "    eligibility = ctr.get('eligibility', [])\n",
    "    adverse_events = ctr.get('adverse_events', [])\n",
    "    results = ctr.get('results', [])\n",
    "  return \"\\n\".join(intervention + eligibility + adverse_events + results)\n",
    "\n",
    "def get_premise_hypothesis(sample, ctrs, use_section=False, include_id=True):\n",
    "  \"\"\"get premise, hypothesis, label, type from a train sample\n",
    "     use_section: whether to export full ctr or section only\n",
    "     full_text_include_id: when exporting full ctr, whether to include section id inside presmise\n",
    "  \"\"\"\n",
    "  sample_type = sample[\"type\"]\n",
    "  section_id = sample[\"section_id\"].lower().replace(\" \", \"_\")\n",
    "\n",
    "  primary_ctr = ctrs[sample[\"primary_id\"]]\n",
    "  if use_section:\n",
    "    primary_text = \"\\n\".join(primary_ctr[section_id])\n",
    "  else:\n",
    "    primary_text = ctr_to_full_text(primary_ctr, include_id)\n",
    "\n",
    "  if sample_type == \"Comparison\":\n",
    "    secondary_ctr = ctrs[sample[\"secondary_id\"]]\n",
    "    if use_section:\n",
    "      secondary_text = \"\\n\".join(secondary_ctr[section_id])\n",
    "    else:\n",
    "      secondary_text = ctr_to_full_text(secondary_ctr, include_id)\n",
    "    premise = (f\"Primary trial evidence are {primary_text}\\n and Secondary \"\n",
    "               + f\"trial evidence are {secondary_text}.\")\n",
    "  else:\n",
    "    premise = (f\"Primary trial evidence are {primary_text}.\")\n",
    "\n",
    "  hypothesis = sample['statement']\n",
    "  label = sample['label']\n",
    "  return premise, hypothesis, label, sample_type\n",
    "\n",
    "def get_premise_hypothesis_by_section(sample, ctrs):\n",
    "  \"\"\"get premise, hypothesis, label, type from a train sample\n",
    "     use_section: whether to export full ctr or section only\n",
    "     full_text_include_id: when exporting full ctr, whether to include section id inside presmise\n",
    "  \"\"\"\n",
    "\n",
    "  premises = {}\n",
    "\n",
    "  primary_ctr = ctrs[sample[\"primary_id\"]]\n",
    "  sample_type = sample[\"type\"]\n",
    "  if sample_type == \"Comparison\":\n",
    "    secondary_ctr = ctrs[sample[\"secondary_id\"]]\n",
    "  else:\n",
    "    secondary_ctr = None\n",
    "\n",
    "  for section_id in ['intervention', 'eligibility', 'results', 'adverse_events']:\n",
    "    section_id = section_id.lower().replace(\" \", \"_\")\n",
    "    primary_text = \"\\n\".join(primary_ctr[section_id])\n",
    "    if sample_type == \"Comparison\":\n",
    "      secondary_text = \"\\n\".join(secondary_ctr[section_id])\n",
    "      premise = (f\"Primary trial evidence are {primary_text}\\n and Secondary \"\n",
    "                + f\"trial evidence are {secondary_text}.\")\n",
    "    else:\n",
    "      premise = (f\"Primary trial evidence are {primary_text}.\")\n",
    "    premises[section_id] = premise\n",
    "\n",
    "  hypothesis = sample['statement']\n",
    "  label = sample['label']\n",
    "\n",
    "  return premises, hypothesis, label, sample_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations, id_to_clinical_trial_record = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_prompt = \"A premise contains four sections: {}\\nA hypothesis describes one of the sections: {}\\nDetermine the most relevant section from the four options: intervention, results, eligibility, adverse_events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(sample_idx):\n",
    "    sample = annotations['validation'][sample_idx]\n",
    "    premise, hypothesis, label, sample_type = get_premise_hypothesis(sample, id_to_clinical_trial_record,\n",
    "                                                                 use_section=False, include_id=True)\n",
    "    prompt = extraction_prompt.format(premise, hypothesis)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    # inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs)\n",
    "    pred = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip().lower()\n",
    "\n",
    "    # observed variations\n",
    "    if pred == 'interventions':\n",
    "        pred = 'intervention'\n",
    "\n",
    "    true_section_id = sample['section_id'].lower().replace(\" \", \"_\")\n",
    "    return pred, true_section_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "too_large = []\n",
    "for i, item in enumerate(tqdm(annotations['validation'])):\n",
    "   try:\n",
    "      pred, label = test(i) \n",
    "   except RuntimeError as e:\n",
    "      if \"out of memory\" in str(e):\n",
    "         too_large.append(i)\n",
    "         gc.collect()\n",
    "         torch.cuda.empty_cache()\n",
    "         continue\n",
    "      \n",
    "   result.append((pred, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: handle \"unknown\"\n",
    "print(classification_report([x[1] for x in result], [x[0] for x in result]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
